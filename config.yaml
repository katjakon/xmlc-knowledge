

model_name: "meta-llama/Llama-3.2-3B" # meta-llama/Llama-3.1-8B #
num_epochs: 1
batch_size: 32
learning_rate: 1e-5
max_grad_norm: 1.0
max_seq_length: 256
eval_steps: 50
logging_steps: 20
save_steps: 500

prompt_config:
  num_prompt_tokens: 50
  at_layer: 20
  hidden_size: 3072
  down_project_size: 512
  dropout: 0.1
  type: "hidden_states" # Choices: "hidden_states", "random"
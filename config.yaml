

model_name: meta-llama/Llama-3.2-3B
sentence_transformer_model: BAAI/bge-m3
experiment_name: baseline-full-run
checkpoint_path: models
num_epochs: 1
batch_size: 32
learning_rate: 5e-5
warmup_rate: 0.05
max_grad_norm: 1.0
max_seq_length: 200
eval_steps: 100
logging_steps: 10
save_steps: 2000
eval_generation: true
train_subsample_ratio: 1.0
validate_subsample_ratio: 0.05
sort_by_freq: true
use_k_freq_labels: 5

prompt_config:
  num_prompt_tokens: 50
  at_layer: 25
  hidden_size: 3072
  down_project_size: 1024
  dropout: 0.1
  type: "hidden_states" # Choices: "hidden_states", "random"


model_name: meta-llama/Llama-3.2-3B
sentence_transformer_model: BAAI/bge-m3
experiment_name: prompt-tuning-text-context-3-label-avg
checkpoint_path: models
graph_path: data/gnd.pickle
dataset_path: datasets/labelwise-3
num_epochs: 1
batch_size: 32
learning_rate: 5e-5
warmup_rate: 0.05
max_grad_norm: 1.0
max_seq_length: 200
max_context_length: 200
eval_steps: 200
logging_steps: 10
save_steps: 4000
eval_generation: true
train_subsample_ratio: 1.0
validate_subsample_ratio: 0.03
sort_by_freq: true
use_k_freq_labels: 0

context:
  context_type: "text" # Choices: "text" or "graph". Use null if no context is needed.
  top_k: 3 # How many instances to retrieve
  hops: 0 # Also include hops in the knowledge graph from retrieved instances
  title_wise: false # Whether to retrieve title-to-label or title-to-title
  relation: null # Choices: null (use all relations) "broader" or "related"
  index_path: "data/label_index.pkl"
  mapping_path: "data/label_mapping.pkl"

prompt_config:
  num_prompt_tokens: 50
  at_layer: 25
  hidden_size: 3072
  down_project_size: 1024
  dropout: 0.1
  type: "context" # Choices: "hidden_states", "random" or "context"
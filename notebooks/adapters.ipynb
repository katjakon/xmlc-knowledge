{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel, BnConfig, init\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from adapters.trainer import AdapterTrainer\n",
    "from transformers.training_args import TrainingArguments \n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "from data_collator import DataCollator\n",
    "from gnd_dataset import GNDDataset\n",
    "from utils import PAD_TOKEN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gnd = pickle.load(open(\"data/gnd.pickle\", \"rb\"))\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "ds_path = \"datasets/no_context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "cm_model = AutoAdapterModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(PAD_TOKEN)\n",
    "\n",
    "bn_name = \"bottleneck-adapter\"\n",
    "config = BnConfig(mh_adapter=True, output_adapter=True, reduction_factor=16, non_linearity=\"silu\")\n",
    "cm_model.add_adapter(bn_name, config=config)\n",
    "cm_model.set_active_adapters(bn_name)\n",
    "cm_model.train_adapter(bn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stack[bottleneck-adapter]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.0.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.0.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.0.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.0.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.0.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.0.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.0.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.1.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.1.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.1.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.1.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.1.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.1.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.1.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.1.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.2.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.2.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.2.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.2.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.2.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.2.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.2.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.2.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.3.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.3.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.3.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.3.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.3.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.3.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.3.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.3.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.4.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.4.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.4.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.4.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.4.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.4.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.4.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.4.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.5.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.5.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.5.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.5.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.5.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.5.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.5.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.5.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.6.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.6.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.6.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.6.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.6.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.6.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.6.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.6.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.7.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.7.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.7.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.7.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.7.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.7.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.7.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.7.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.8.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.8.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.8.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.8.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.8.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.8.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.8.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.8.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.9.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.9.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.9.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.9.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.9.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.9.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.9.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.9.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.10.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.10.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.10.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.10.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.10.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.10.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.10.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.10.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.11.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.11.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.11.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.11.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.11.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.11.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.11.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.11.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.12.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.12.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.12.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.12.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.12.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.12.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.12.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.12.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.13.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.13.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.13.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.13.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.13.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.13.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.13.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.13.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.14.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.14.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.14.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.14.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.14.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.14.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.14.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.14.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.15.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.15.attention_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.15.attention_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.15.attention_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n",
      "model.layers.15.output_adapters.adapters.bottleneck-adapter.adapter_down.0.weight torch.Size([128, 2048])\n",
      "model.layers.15.output_adapters.adapters.bottleneck-adapter.adapter_down.0.bias torch.Size([128])\n",
      "model.layers.15.output_adapters.adapters.bottleneck-adapter.adapter_up.weight torch.Size([2048, 128])\n",
      "model.layers.15.output_adapters.adapters.bottleneck-adapter.adapter_up.bias torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "cm_model.train_adapter(bn_name)\n",
    "trainable = 0\n",
    "for n, p in cm_model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "        print(f\"{n} {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaAttentionWithAdapters(\n",
       "          (q_proj): LoRALinearTorch(\n",
       "            in_features=2048, out_features=2048, bias=False\n",
       "            (shared_parameters): ModuleDict()\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (k_proj): LoRALinearTorch(\n",
       "            in_features=2048, out_features=512, bias=False\n",
       "            (shared_parameters): ModuleDict()\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (v_proj): LoRALinearTorch(\n",
       "            in_features=2048, out_features=512, bias=False\n",
       "            (shared_parameters): ModuleDict()\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (prefix_tuning): PrefixTuningLayer(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): LoRALinearTorch(\n",
       "            in_features=2048, out_features=8192, bias=False\n",
       "            (shared_parameters): ModuleDict()\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (down_proj): LoRALinearTorch(\n",
       "            in_features=8192, out_features=2048, bias=False\n",
       "            (shared_parameters): ModuleDict()\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (reft_layer): ReftLayer(\n",
       "          (refts): ModuleDict()\n",
       "        )\n",
       "        (attention_adapters): BottleneckLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): BottleneckLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "    (invertible_adapters): ModuleDict()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'add_causal_lm_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m config = BnConfig(mh_adapter=\u001b[38;5;28;01mTrue\u001b[39;00m, output_adapter=\u001b[38;5;28;01mTrue\u001b[39;00m, reduction_factor=\u001b[32m16\u001b[39m, non_linearity=\u001b[33m\"\u001b[39m\u001b[33msilu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m init(cm_model)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mcm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_causal_lm_head\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mlm_head\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# cm_model.set_active_adapters(bn_name)\u001b[39;00m\n\u001b[32m     11\u001b[39m adapter_name = cm_model.load_adapter(\u001b[33m\"\u001b[39m\u001b[33madapter_model/checkpoint-8000/bottleneck-adapter\u001b[39m\u001b[33m\"\u001b[39m, set_active=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/xmlc-knowledge/ki-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LlamaForCausalLM' object has no attribute 'add_causal_lm_head'"
     ]
    }
   ],
   "source": [
    "\n",
    "cm_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(PAD_TOKEN)\n",
    "\n",
    "bn_name = \"bottleneck-adapter\"\n",
    "config = BnConfig(mh_adapter=True, output_adapter=True, reduction_factor=16, non_linearity=\"silu\")\n",
    "init(cm_model)\n",
    "cm_model.add_causal_lm_head(\"lm_head\")\n",
    "# cm_model.set_active_adapters(bn_name)\n",
    "adapter_name = cm_model.load_adapter(\"adapter_model/checkpoint-8000/bottleneck-adapter\", set_active=True)\n",
    "# cm_model.set_active_adapters(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    graph=gnd,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GNDDataset(\n",
    "    data_dir=ds_path,\n",
    "    gnd_graph=gnd,\n",
    "    load_from_disk=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titel: Synökologische Studien zum simultanen Befall von Winterweizen (Triticum aestivum L.) mit Aphiden und getreidepathogenen Pilzen. Schlagwörter: \n",
      "Titel: Regeln für die Ansetzung von Körperschaftsnamen RAK-Körperschaften ; Bestimmungen d. RAK-WB, Grundbestimmungen d. RAK-ÖB, GKD-Erl. u. -Anh.. Schlagwörter: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titel: Kleines Fernsprechbuch für Lübeck und Umgegend. Schlagwörter: \n",
      "Titel: Spurensuche im Schlossmuseum Jever Beiträge zur Provenienzforschung, Sammlungs- und Vereinsgeschichte. Schlagwörter: \n",
      "Titel: Der Bitcoin-Standard die dezentrale Alternative zum Zentralbankensystem. Schlagwörter: \n",
      "Titel: Samuel Tillerman, der Läufer. Schlagwörter: 4. Reiterter; Läder; Schuljung; Schüler; Schuljä\n",
      "Titel: Charakterisierung der Schilddrüsenfunktion und Nachweis eines Promotordefektes als Ursache des kompletten Thyroxin-bindenden Globulin-Mangels beim Hund. Schlagwörter: \n",
      "Titel: Indikatoren und Verpackungen für in der Endverpackung zu sterilisierende Medizinprodukte Normen. Schlagwörter: \n",
      "Titel: Das Findelkind vom Watt. Schlagwörter: 5. Sammlungsliteratur; Soldat von Soldat von Soldat von Soldat\n",
      "Titel: Pronomen und Pronominalklitika im Cimbro Untersuchungen zum grammatischen Wandel einer deutschen Minderheitensprache in romanischer Umgebung. Schlagwörter: \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "num_instances = 10\n",
    "for instance in ds[\"validate\"]:\n",
    "    count += 1\n",
    "    if count > num_instances:\n",
    "        break\n",
    "    gold_labels = instance[\"label-names\"]\n",
    "    label_string = instance[\"label-string\"]\n",
    "    instance_batch = {k: torch.tensor(v) for k, v in instance.items() if k in [\"attention_mask\", \"input_ids\", \"seq_lengths\"]}\n",
    "    input_ids = instance_batch[\"input_ids\"]\n",
    "    attention_mask = instance_batch[\"attention_mask\"]\n",
    "    sequence_length = instance_batch[\"seq_lengths\"]\n",
    "    input_ids = input_ids[:sequence_length]\n",
    "    attention_mask = attention_mask[:sequence_length]\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids.unsqueeze(0),\n",
    "        \"attention_mask\": attention_mask.unsqueeze(0),\n",
    "    }\n",
    "    output = cm_model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'================================================================================\\nName                     Architecture         #Param      %Param  Active   Train\\n--------------------------------------------------------------------------------\\nbottleneck-adapter       bottleneck       16,846,848       1.363       1       1\\n--------------------------------------------------------------------------------\\nFull model                              1,235,814,400     100.000               1\\n================================================================================'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_model.adapter_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
